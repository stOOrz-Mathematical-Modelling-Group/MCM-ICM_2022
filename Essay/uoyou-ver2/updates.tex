\documentclass{article}
\usepackage{amsfonts}
\usepackage{makecell}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{ifthen}
\usepackage{wrapfig}
\usepackage{array}
\usepackage{colortbl}
\usepackage{fullpage}
\usepackage[table]{xcolor}
\usepackage{subfigure}
\usepackage{caption}
\usepackage{float}
\usepackage{ctex}
\usepackage{appendix}
\usepackage{shorttoc}
\usepackage{pagenote}
\usepackage{setspace}
\usepackage{geometry}
\usepackage{indentfirst}
\usepackage{fullpage}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{minipage-marginpar}
\usepackage{hyperref}
\usepackage{palatino}
\usepackage{setspace}
\usepackage{subfigure}
\usepackage{picinpar}
\usepackage{newtxtext}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{extarrows}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{paralist}
\usepackage{newtxmath} % must come after amsXXX
\usepackage{lastpage}
\usepackage{clrscode3e}
\begin{document}
\begin{algorithm}
	\caption{Q-Learning}
	\begin{algorithmic}
		\STATE initialize Q-table
		\STATE initialize S
		\STATE $f_{\mathrm{fatigue}}\leftarrow 0$
		\FOR{ athlete $i$:}
    	\STATE randomly initialize the parameter vector $\vec{v}=\left( v_1,v_2,...,v_k \right)$ of $i$ \# showing his parameters
    	\STATE his current strategy vector $\vec{v\left(t\right)}=\left( v_1\left(t\right),v_2\left(t\right),...,v_k\left(t\right) \right)$ \# showing his strategy
		\ENDFOR
		\WHILE{not at the ending point:}
    	\STATE \# change his strategy vector $\vec{v\left(t\right)}$ according to Q-table
    	\STATE choose a strategy $A$ based on $\vec{v\left(t\right)}$ and P
​ 		 \STATE calculate the expected (best) strategy for the current terrain based on previous calculations $\vec{v}^{\Theta}=\left( v_{1}^{\Theta},v_{2}^{\Theta},...,v_{k}^{\Theta} \right) $
​		 \STATE $\Delta ^{\Theta}v\leftarrow \sqrt{\dfrac{\sum\limits_{i=1}^k{\left( v_i-v_{i}^{\Theta} \right) ^2}}{k}}\xlongequal{\mathrm{def}}f_{\Delta}\left( \vec{v} \right) $
​    	 \STATE $Q(S,A)\gets (1-\underset{\mathrm{learning}\:\:\mathrm{rate}}{\underbrace{\alpha }})Q(S,A)+\underset{\mathrm{learning}\:\:\mathrm{rate}}{\underbrace{\alpha }}[\underset{\mathrm{reward}}{\underbrace{R}}+\underset{\mathrm{discount}\:\:\mathrm{factor}}{\underbrace{\gamma }}\cdot \mathrm{inverse}\:\:\mathrm{strategy}\left( \nabla f_{\Delta}\left( \vec{v} \right) \right) -Q(S,A)]$ \# learning rate $\alpha$ can change due to $f_{\mathrm{fatigue}}$
​     	 \STATE \# when $f_{\mathrm{fatigue}}$ increases, $\alpha$ decreases
​    	 \STATE Update Q-table
​    	 \STATE Update $f_{\mathrm{fatigue}}$
​    	 \STATE Update $\alpha$
		\ENDWHILE
		\STATE show the best strategy
	\end{algorithmic}
\end{algorithm}
\end{document}